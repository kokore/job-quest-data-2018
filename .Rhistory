ll
library(tm)
library(SnowballC)
install.packages("NLP")
install.packages("NLP")
library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(tm)
install.packages(NLP)
install.packages("NLP")
install.packages("NLP")
library(tm)
library(SnowballC)
library(wordcloud)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(ggplot2)
setwd('quanteda')
setwd('/home/surapol/Downloads/job-quest-data-2018')
library(tm)
library(ggplot2)
library(quanteda)
install.packages('library(quanteda)')
install.packages('quanteda')
library(tm)
library(ggplot2)
# load data
data <- read.csv("sample-data.csv")
#remove html tags
data$name <- gsub("<[^>]+>", "", data$name)
data$introduction <- gsub("<[^>]+>", "", data$introduction)
data$itinerary <- gsub("<[^>]+>", "", data$itinerary )
data$destination_location <- gsub("<[^>]+>", "", data$destination_location)
data$name <- gsub("[^\x20-\x7E]", "", data$name)
data$introduction <- gsub("[^\x20-\x7E]", "", data$introduction)
data$itinerary <- gsub("[^\x20-\x7E]", "", data$itinerary )
data$destination_location <- gsub("[^\x20-\x7E]", "", data$destination_location)
#get data and remove  location table
data1 <- subset(data , select = -destination_location)
#get all location and remove duplicated
location <- data$destination_location
location <- location[!duplicated(location)]
# change data to corpus
docs <- Corpus(VectorSource(data1))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# change data to termdocment
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 4)
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 20)
#find assocs by freq terms
data4 <- findAssocs(dtm, data3, corlimit = 0.9)
data3
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 15)
data3
#find assocs by freq terms
data4 <- findAssocs(dtm, data3, corlimit = 0.9)
data3
data3
data4
summary(data4)
data4
findAssocs(dtm,i,corlimit = 0.9)
#find assocs by freq terms
for (i in data3) {
findAssocs(dtm,i,corlimit = 0.9)
}
findAssocs(dtm,i,corlimit = 0.9)
#find assocs by freq terms
for (i in data3) {
findAssocs(dtm,i,corlimit = 0.9)
}
#find assocs by freq terms
for (i in data3) {
print(findAssocs(dtm,i,corlimit = 0.9))
}
#find assocs by freq terms
for (i in data3) {
print(findAssocs(dtm,i,corlimit = 0.95))
}
data3
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 16)
data3
findFreqTerms(dtm, lowfreq = 17)
findFreqTerms(dtm, lowfreq = 18)
findFreqTerms(dtm, lowfreq = 19)
findFreqTerms(dtm, lowfreq = 20)
findFreqTerms(dtm, lowfreq = 25)
findFreqTerms(dtm, lowfreq = 15)
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 15)
#find assocs by freq terms
for (i in data3) {
print(findAssocs(dtm,i,corlimit = 0.95))
}
data4$along
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 15)
#find assocs by freq terms
data4 <- findAssocs(dtm, data3, corlimit = 0.95)
data3
data4$along
plot(dtm, corThreshold = 0.80)
install.packages('Rgraphviz')
source("https://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
plot(dtm, corThreshold = 0.80)
plot(data4, corThreshold = 0.80)
f <- matrix (0, ncol=nrow(data4), nrow=nrow(data4))
f <- matrix (ncol=nrow(data4), nrow=nrow(data4))
f <- matrix (0,ncol=ncol(data4), nrow=nrow(data4))
ncol(data4)
summary(data4)
typeof(data4)
data4
f <- matrix (0, ncol=nrow(dtm), nrow=nrow(dtm))
colnames (f) <- rownames(dtm)
rownames (f) <- rownames(dtm)
for (i in rownames (dtm)) {
ff <- findAssocs (dtm,i,0)
for  (j in rownames (ff)) {
f[j,i]=ff[j,]
}
}
# find freq terms
findFreqTerms(dtm, lowfreq = 15)
# find freq terms
findFreqTerms(dtm, lowfreq = 15)
#find assocs by freq terms
findAssocs(dtm, location , corlimit = 0.95)
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 15)
#find assocs by freq terms
data4 <- findAssocs(dtm, data3, corlimit = 0.95)
f <- matrix (0, ncol=nrow(dtm), nrow=nrow(dtm))
colnames (f) <- rownames(dtm)
rownames (f) <- rownames(dtm)
View(f)
nrow(data4)
ncol(data4)
nrow(dtm)
View(data4)
nrow(dtm)
ncol(dtm)
data4
View(data4)
nrow(dtm)
ncol(dtm)
data3
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 10)
data3
library(tm)
library(ggplot2)
# load data
data <- read.csv("sample-data.csv")
#remove html tags
data$name <- gsub("<[^>]+>", "", data$name)
data$introduction <- gsub("<[^>]+>", "", data$introduction)
data$itinerary <- gsub("<[^>]+>", "", data$itinerary )
data$destination_location <- gsub("<[^>]+>", "", data$destination_location)
data$name <- gsub("[^\x20-\x7E]", "", data$name)
data$introduction <- gsub("[^\x20-\x7E]", "", data$introduction)
data$itinerary <- gsub("[^\x20-\x7E]", "", data$itinerary )
data$destination_location <- gsub("[^\x20-\x7E]", "", data$destination_location)
#get data and remove  location table
data1 <- subset(data , select = -destination_location)
#get all location and remove duplicated
location <- data$destination_location
location <- location[!duplicated(location)]
# change data to corpus
docs <- Corpus(VectorSource(data1))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# change data to termdocment
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
# find freq terms
data3 <- findFreqTerms(dtm, lowfreq = 10)
#find assocs by freq terms
data4 <- findAssocs(dtm, data3, corlimit = 0.95)
data4
data5 <- rownames(findAssocs(dtm, data3, corlimit = 0.95))[1:20]
data4$ayutthaya
